# Paper Submission Portal

A microservices-based plagiarism detection system for research paper submissions with event-driven architecture and stream processing.

## üìã Infrastructure Status

**Last Updated:** 2025-11-23  
**Readiness:** 70% - Core infrastructure deployed, security improvements recommended  
**Terraform Validation:** See `TERRAFORM_ISSUES_AND_FIXES.md` for detailed status

### ‚úÖ Deployed Components
- GKE cluster with custom VPC (cross-cloud connectivity foundation)
- Cloud SQL PostgreSQL with Workload Identity
- AWS MSK Kafka cluster with security group rules
- AWS Lambda for PDF extraction
- Managed Flink with required JAR validation
- Observability stack (Prometheus/Grafana/Loki)
- ArgoCD for GitOps

### ‚ö†Ô∏è Pending Improvements
- Lambda IAM policy (too permissive)
- Kafka TLS verification (hard-coded for dev)
- Cloud SQL backups & deletion protection
- Remote state backend for team collaboration

---

## Architecture

This project follows a **microservices architecture** with embedded stream processing.

### Data Flow Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          PRODUCER SERVICES                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Submission      ‚îÇ                    ‚îÇ  Plagiarism      ‚îÇ
    ‚îÇ  Service         ‚îÇ                    ‚îÇ  Service         ‚îÇ
    ‚îÇ  (Port 8002)     ‚îÇ                    ‚îÇ  (Port 8003)     ‚îÇ
    ‚îÇ                  ‚îÇ                    ‚îÇ                  ‚îÇ
    ‚îÇ ‚Ä¢ Business logic ‚îÇ                    ‚îÇ ‚Ä¢ Plagiarism     ‚îÇ
    ‚îÇ ‚Ä¢ Validation     ‚îÇ                    ‚îÇ   detection      ‚îÇ
    ‚îÇ ‚Ä¢ Persistence    ‚îÇ                    ‚îÇ ‚Ä¢ Severity calc  ‚îÇ
    ‚îÇ                  ‚îÇ                    ‚îÇ ‚Ä¢ Risk flagging  ‚îÇ
    ‚îÇ ‚Ä¢ aiokafka       ‚îÇ                    ‚îÇ ‚Ä¢ AI detection   ‚îÇ
    ‚îÇ   stream proc    ‚îÇ                    ‚îÇ                  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ ‚Ä¢ aiokafka       ‚îÇ
             ‚îÇ                              ‚îÇ   stream proc    ‚îÇ
             ‚îÇ Produces                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                                       ‚îÇ
             ‚îÇ                                       ‚îÇ Produces
             ‚ñº                                       ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Kafka Topic:       ‚îÇ            ‚îÇ Kafka Topic:           ‚îÇ
    ‚îÇ paper_uploaded     ‚îÇ            ‚îÇ plagiarism_checked     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                                 ‚îÇ
             ‚îÇ                                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            ‚îÇ  EMBEDDED STREAM PROCESSORS     ‚îÇ                 ‚îÇ
‚îÇ            ‚îÇ  (aiokafka - async)             ‚îÇ                 ‚îÇ
‚îÇ            ‚îÇ                                 ‚îÇ                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Submission Service     ‚îÇ      ‚îÇ Plagiarism Service      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Stream Processor       ‚îÇ      ‚îÇ Stream Processor        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                        ‚îÇ      ‚îÇ                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Consume topic        ‚îÇ      ‚îÇ ‚Ä¢ Consume topic         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Add metadata         ‚îÇ      ‚îÇ ‚Ä¢ Add metadata          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Text length calc     ‚îÇ      ‚îÇ ‚Ä¢ Passthrough enriched  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Async processing     ‚îÇ      ‚îÇ   data from service     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ             ‚îÇ                               ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ Writes to                     ‚îÇ Writes to
              ‚ñº                               ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Kafka Topic:        ‚îÇ         ‚îÇ Kafka Topic:            ‚îÇ
    ‚îÇ paper_uploaded_     ‚îÇ         ‚îÇ plagiarism_checked_     ‚îÇ
    ‚îÇ processed           ‚îÇ         ‚îÇ processed               ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ                               ‚îÇ
              ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ         ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        ANALYTICS      ‚îÇ         ‚îÇ                           ‚îÇ
‚îÇ        SERVICE        ‚îÇ         ‚îÇ                           ‚îÇ
‚îÇ                       ‚ñº         ‚ñº                           ‚îÇ
‚îÇ            ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó                    ‚îÇ
‚îÇ            ‚ïë Flink Stream Processor    ‚ïë                    ‚îÇ
‚îÇ            ‚ïë (Windowed Aggregations)   ‚ïë                    ‚îÇ
‚îÇ            ‚ïë                           ‚ïë                    ‚îÇ
‚îÇ            ‚ïë ‚Ä¢ Reads both topics       ‚ïë                    ‚îÇ
‚îÇ            ‚ïë ‚Ä¢ 5-min tumbling windows  ‚ïë                    ‚îÇ
‚îÇ            ‚ïë ‚Ä¢ Union streams           ‚ïë                    ‚îÇ
‚îÇ            ‚ïë ‚Ä¢ Aggregates metrics:     ‚ïë                    ‚îÇ
‚îÇ            ‚ïë   - Submission counts     ‚ïë                    ‚îÇ
‚îÇ            ‚ïë   - Avg plagiarism score  ‚ïë                    ‚îÇ
‚îÇ            ‚ïë   - High-risk count       ‚ïë                    ‚îÇ
‚îÇ            ‚ïë   - AI detection count    ‚ïë                    ‚îÇ
‚îÇ            ‚ïë                           ‚ïë                    ‚îÇ
‚îÇ            ‚ïë ‚ö†Ô∏è  Flink used ONLY for   ‚ïë                    ‚îÇ
‚îÇ            ‚ïë    time-based windowing   ‚ïë                    ‚îÇ
‚îÇ            ‚ïë    (unavoidable)          ‚ïë                    ‚îÇ
‚îÇ            ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚î¨‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù                    ‚îÇ
‚îÇ                        ‚îÇ                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ Writes aggregated data
                         ‚ñº
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ Kafka Topic:       ‚îÇ
                ‚îÇ analytics_window   ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚îÇ Consumed by
                         ‚îÇ Analytics Service
                         ‚îÇ (aiokafka)
                         ‚ñº
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ Analytics Service  ‚îÇ
                ‚îÇ REST API           ‚îÇ
                ‚îÇ (Port 8004)        ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  OTHER CONSUMER SERVICES                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Notification    ‚îÇ  ‚îÇ  Notification    ‚îÇ
‚îÇ  Service         ‚îÇ  ‚îÇ  Service         ‚îÇ
‚îÇ  (Port 8005)     ‚îÇ  ‚îÇ  (Port 8005)     ‚îÇ
‚îÇ                  ‚îÇ  ‚îÇ                  ‚îÇ
‚îÇ  Consumes:       ‚îÇ  ‚îÇ  Consumes:       ‚îÇ
‚îÇ  paper_uploaded_ ‚îÇ  ‚îÇ  plagiarism_     ‚îÇ
‚îÇ  processed       ‚îÇ  ‚îÇ  checked_        ‚îÇ
‚îÇ                  ‚îÇ  ‚îÇ  processed       ‚îÇ
‚îÇ  ‚Ä¢ Upload alerts ‚îÇ  ‚îÇ  ‚Ä¢ Smart alerts  ‚îÇ
‚îÇ  ‚Ä¢ Text length   ‚îÇ  ‚îÇ    with severity ‚îÇ
‚îÇ    in notif      ‚îÇ  ‚îÇ  ‚Ä¢ Risk-based    ‚îÇ
‚îÇ                  ‚îÇ  ‚îÇ    messages      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Services

1. **Gateway** (`services/gateway/`) - Port 8000
   - Web frontend and API gateway
   - User interface for students and faculty

2. **User Service** (`services/users_service/`) - Port 8001
   - Manages users (students and faculty)
   - CRUD operations for user accounts

3. **Submission Service** (`services/submission_service/`) - Port 8002
   - Handles research paper submissions
   - **Business logic**: Submission validation, file handling
   - **Stream processor** (aiokafka): Enriches events with metadata
   - Emits to: `paper_uploaded` ‚Üí `paper_uploaded_processed`

4. **Plagiarism Service** (`services/plagiarism_service/`) - Port 8003
   - **Business logic**: Plagiarism detection, severity categorization, risk flagging, AI detection
   - **Stream processor** (aiokafka): Forwards enriched events
   - Emits to: `plagiarism_checked` ‚Üí `plagiarism_checked_processed`

5. **Analytics Service** (`services/analytics_service/`) - Port 8004
   - **Business logic**: Analytics queries and trending
   - **Stream processor** (Apache Flink): Time-based windowed aggregations (5-min windows)
   - Consumes: Both processed topics ‚Üí Produces: `analytics_window`

6. **Notification Service** (`services/notification_service/`) - Port 8005
   - Sends notifications to users
   - Consumes processed topics for smart notifications

### Architecture Principles

#### 1. Business Logic in Services ‚úÖ
- ‚úÖ Severity calculation in `plagiarism_service/app/engine.py`
- ‚úÖ Risk flagging in `plagiarism_service/app/engine.py`
- ‚úÖ AI detection in `plagiarism_service/app/engine.py`
- ‚úÖ Single source of truth for domain logic

#### 2. Async Stream Processing (aiokafka) ‚úÖ
- ‚úÖ Submission service: Simple consume ‚Üí enrich ‚Üí produce
- ‚úÖ Plagiarism service: Simple passthrough with metadata
- ‚úÖ No blocking operations, fully async

#### 3. Flink ONLY for Time Windowing ‚ö†Ô∏è
- ‚úÖ Analytics service: 5-minute tumbling windows
- ‚úÖ Event time processing with watermarks
- ‚úÖ State management for window aggregations
- ‚ùå NOT used for business logic
- ‚ùå NOT used for simple consume/produce

#### 4. Separation of Concerns
**Services own:**
- Domain logic
- Data validation
- API endpoints
- Persistence

**Stream processors own:**
- Kafka consumption/production
- Minimal enrichment (metadata)
- Time-based aggregations (Flink only)

### Kafka Topics

| Topic | Producer | Consumer |
|-------|----------|----------|
| `paper_uploaded` | submission-service | submission stream processor |
| `paper_uploaded_processed` | submission stream processor | notification-service, analytics Flink |
| `plagiarism_checked` | plagiarism-service | plagiarism stream processor |
| `plagiarism_checked_processed` | plagiarism stream processor | notification-service, analytics Flink |
| `analytics_window` | analytics Flink processor | analytics-service |

### Technology Choices

| Component | Technology | Justification |
|-----------|------------|---------------|
| Submission enrichment | aiokafka | Simple async consume/produce |
| Plagiarism passthrough | aiokafka | Simple async consume/produce |
| Analytics windowing | Apache Flink | Time-based windows unavoidable |
| Business logic | FastAPI services | Domain expertise in services |
| Event schema | Pydantic | Type safety and validation |

### Shared Libraries

- **libs/events/** - Event schemas and Kafka utilities
  - Pydantic models for inter-service communication
  - Async Kafka producer/consumer clients (aiokafka)

- **config/** - Centralized configuration
  - Environment-based settings
  - Service URLs and Kafka configuration

## Project Structure

```
Project/
‚îú‚îÄ‚îÄ services/                           # Microservices
‚îÇ   ‚îú‚îÄ‚îÄ gateway/                        # Web UI (Port 8000)
‚îÇ   ‚îú‚îÄ‚îÄ users_service/                  # User management (Port 8001)
‚îÇ   ‚îú‚îÄ‚îÄ submission_service/             # Paper submissions (Port 8002)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ app/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ main.py                 # FastAPI app
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ stream_processor.py     # aiokafka stream processor
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ store.py                # Data layer
‚îÇ   ‚îú‚îÄ‚îÄ plagiarism_service/             # Plagiarism detection (Port 8003)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ app/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ main.py                 # FastAPI app
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ engine.py               # Business logic (severity, risk, AI)
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ stream_processor.py     # aiokafka stream processor
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ store.py                # Data layer
‚îÇ   ‚îú‚îÄ‚îÄ analytics_service/              # Analytics (Port 8004)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ app/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ main.py                 # FastAPI app
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ stream_processor.py     # Flink windowed aggregations
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ store.py                # Data layer
‚îÇ   ‚îî‚îÄ‚îÄ notification_service/           # Notifications (Port 8005)
‚îú‚îÄ‚îÄ libs/                               # Shared libraries
‚îÇ   ‚îî‚îÄ‚îÄ events/
‚îÇ       ‚îú‚îÄ‚îÄ schemas.py                  # Pydantic event models
‚îÇ       ‚îî‚îÄ‚îÄ kafka.py                    # Async Kafka clients
‚îú‚îÄ‚îÄ config/                             # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ settings.py                     # Centralized settings
‚îÇ   ‚îî‚îÄ‚îÄ logging.py                      # Logging setup
‚îú‚îÄ‚îÄ scripts/                            # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ seed_data.py                    # Seed test data
‚îÇ   ‚îî‚îÄ‚îÄ test_workflow.py                # Test end-to-end flow
‚îú‚îÄ‚îÄ k8s/                                # Kubernetes manifests
‚îú‚îÄ‚îÄ ARCHITECTURE.txt                    # Data flow diagram
‚îî‚îÄ‚îÄ requirements.txt                    # Python dependencies
```

## Setup

### Prerequisites

- Python 3.12+
- Virtual environment (recommended)

### Installation

1. **Create and activate virtual environment:**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt

### Managed Flink (AWS Kinesis Data Analytics) Migration

This project now uses **Amazon Managed Flink (Kinesis Data Analytics for Apache Flink)** instead of an EMR cluster for windowed stream aggregations.

Key changes:
- Terraform: EMR module replaced by `aws_managed_flink` (resource `aws_kinesisanalyticsv2_application`).
- IAM: A dedicated execution role grants access to S3 (job artifact), CloudWatch Logs, and MSK metadata APIs.
- Code: A new client `libs/flink/managed_client.py` provides async wrappers for application lifecycle (start/stop/status).
- Dependencies: Added `boto3` to `requirements.txt`.

Flink job packaging:
1. Build a shaded/uber JAR for the Flink job (Scala/Java) OR PyFlink ZIP bundle.
2. Upload it to S3, e.g. `s3://your-bucket/path/job.jar`.
3. Set Terraform variable `flink_job_jar` to that S3 URI and apply.

Terraform apply (example):
```bash
terraform init
terraform plan -var="flink_job_jar=s3://your-bucket/path/job.jar" \
               -var="aws_region=us-east-1" \
               -var="gcp_project_id=your-project-id"
terraform apply -auto-approve
```

Runtime operations (Python example):
```python
from libs.flink.managed_client import ManagedFlinkClient
client = ManagedFlinkClient(region="us-east-1", application_name="dev-managed-flink")
apps = await client.list_applications()
detail = await client.describe_application()
await client.start_application()  # start if READY
```

Advantages of Managed Flink:
- No cluster sizing/patching overhead.
- Auto-scaling parallelism configuration.
- Integrated monitoring & snapshots.
- Simplified IAM boundary versus multi-service EMR roles.
   ```

### Running Services

Each service can be run independently:

```bash
# User Service
uvicorn services.users_service.app.main:app --port 8001

# Submission Service
uvicorn services.submission_service.app.main:app --port 8002

# Plagiarism Service
uvicorn services.plagiarism_service.app.main:app --port 8003

# Analytics Service
uvicorn services.analytics_service.app.main:app --port 8004

# Notification Service
uvicorn services.notification_service.app.main:app --port 8005

# Gateway (Web UI)
uvicorn services.gateway.app.main:app --port 8000
```

### Running Tests

```bash
# Run all tests
pytest -q

# Run specific service tests
pytest services/users_service/tests/ -v
pytest services/submission_service/tests/ -v
```

## Configuration

Environment variables can be set in a `.env` file at the project root:

```env
# Service URLs
USERS_SERVICE_URL=http://localhost:8001
SUBMISSION_SERVICE_URL=http://localhost:8002
PLAGIARISM_SERVICE_URL=http://localhost:8003
ANALYTICS_SERVICE_URL=http://localhost:8004
NOTIFICATION_SERVICE_URL=http://localhost:8005
GATEWAY_URL=http://localhost:8000

# Kafka
KAFKA_BROKER=localhost:9092

# Flink
FLINK_JOBMANAGER=http://localhost:8081

# Security
SECRET_KEY=your-secret-key-here
```

## Event-Driven Architecture

Services communicate via Kafka topics with embedded stream processors.

### Kafka Topics Flow

```
submission_service
    ‚îÇ
    ‚îú‚îÄ> paper_uploaded (raw)
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> [submission stream processor - aiokafka]
    ‚îÇ       ‚îÇ       ‚Ä¢ Adds metadata (processed_at, text_length)
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îî‚îÄ> paper_uploaded_processed
    ‚îÇ               ‚îÇ
    ‚îÇ               ‚îî‚îÄ> notification_service (consumes)
    ‚îÇ               ‚îî‚îÄ> analytics Flink processor (consumes)

plagiarism_service
    ‚îÇ
    ‚îú‚îÄ> plagiarism_checked (raw, enriched by service)
    ‚îÇ       ‚Ä¢ severity: high/medium/low
    ‚îÇ       ‚Ä¢ requires_review: boolean
    ‚îÇ       ‚Ä¢ ai_generated_likely: boolean
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îú‚îÄ> [plagiarism stream processor - aiokafka]
    ‚îÇ       ‚îÇ       ‚Ä¢ Adds processing metadata
    ‚îÇ       ‚îÇ
    ‚îÇ       ‚îî‚îÄ> plagiarism_checked_processed
    ‚îÇ               ‚îÇ
    ‚îÇ               ‚îî‚îÄ> notification_service (consumes)
    ‚îÇ               ‚îî‚îÄ> analytics Flink processor (consumes)

analytics_service
    ‚îÇ
    ‚îú‚îÄ> [Flink stream processor]
    ‚îÇ       ‚Ä¢ Reads: paper_uploaded_processed + plagiarism_checked_processed
    ‚îÇ       ‚Ä¢ 5-minute tumbling windows
    ‚îÇ       ‚Ä¢ Aggregates: counts, averages, high-risk counts
    ‚îÇ
    ‚îî‚îÄ> analytics_window
            ‚îÇ
            ‚îî‚îÄ> analytics_service REST API (consumes)
```

### Why Different Technologies?

| Service | Stream Tech | Justification |
|---------|-------------|---------------|
| Submission | aiokafka | Simple consume ‚Üí enrich ‚Üí produce |
| Plagiarism | aiokafka | Simple passthrough (logic in service) |
| Analytics | Apache Flink | Time-based windowing requires specialized infrastructure |

**Flink is used ONLY in analytics service** because implementing proper time-based windowing, watermarks, state management, and late data handling manually would be overly complex.

## API Endpoints

### Gateway (Port 8000)
- `GET /` - Home page
- `GET /login` - Login page
- `POST /login` - Login handler
- `GET /dashboard/student` - Student dashboard
- `GET /dashboard/faculty` - Faculty dashboard

### User Service (Port 8001)
- `POST /users` - Create user
- `GET /users/{id}` - Get user by ID
- `GET /users` - List all users
- `PUT /users/{id}` - Update user
- `DELETE /users/{id}` - Delete user

### Submission Service (Port 8002)
- `POST /submissions` - Create paper submission
- `GET /submissions/{id}` - Get paper submission
- `GET /submissions/user/{user_id}` - Get user's paper submissions

### Plagiarism Service (Port 8003)
- `POST /check` - Check research paper for plagiarism
- `GET /results/{submission_id}` - Get plagiarism results
- `GET /stats` - Get plagiarism statistics

**Returns enriched plagiarism results with:**
- `severity`: "low" | "medium" | "high" (calculated in `engine.py`)
- `requires_review`: boolean (true if internal_score >= 0.5)
- `ai_generated_likely`: boolean (true if ai_probability >= 0.7)

### Analytics Service (Port 8004)
- `GET /analytics/latest` - Get latest analytics window
- `GET /analytics/history` - Get analytics history

### Notification Service (Port 8005)
- `POST /notify` - Send notification

## Development

### Adding a New Service

1. Create service directory under `services/`
2. Add `app/main.py` with FastAPI app and lifespan management
3. If consuming Kafka, add `app/stream_processor.py` (use aiokafka for simple cases)
4. Add business logic in domain modules (e.g., `engine.py`, `store.py`)
5. Add tests in `tests/`
6. Import from `libs.events` for shared schemas

### Best Practices

- ‚úÖ **Business logic in services** - Keep domain logic in service code, not stream processors
- ‚úÖ **Use aiokafka first** - Only use Flink if you need time-based windowing
- ‚úÖ **Event schemas** - Use Pydantic models from `libs.events.schemas`
- ‚úÖ **Async everywhere** - Use async/await for all I/O operations
- ‚úÖ **Loose coupling** - Services communicate via events, not direct HTTP calls
- ‚úÖ **Test independently** - Each service should be testable in isolation

### Stream Processing Guidelines

**When to use aiokafka:**
- Simple consume ‚Üí process ‚Üí produce patterns
- Stateless transformations
- Metadata enrichment

**When to use Flink:**
- Time-based windowing (tumbling, sliding, session windows)
- Complex event processing with state
- Event time processing with watermarks
- When you need exactly-once semantics with state

## Kubernetes Deployment

Kubernetes manifests are provided in `k8s/`:

```bash
# Deploy all services
kubectl apply -f k8s/base/
kubectl apply -f k8s/gateway/
kubectl apply -f k8s/users-service/
kubectl apply -f k8s/submission-service/
kubectl apply -f k8s/plagiarism-service/
kubectl apply -f k8s/analytics-service/
kubectl apply -f k8s/notification-service/

# Or use the deployment script
bash deploy-k8s.sh
```

## Important Notes

### Why aiokafka Instead of Flink for Most Services?

**aiokafka is preferred because:**
- ‚úÖ Fully async/await compatible (non-blocking)
- ‚úÖ Simple consume ‚Üí process ‚Üí produce patterns
- ‚úÖ Lower resource overhead
- ‚úÖ Easier to test and debug
- ‚úÖ No complex infrastructure needed

**Flink is ONLY used when:**
- ‚ö†Ô∏è Time-based windowing is required (tumbling, sliding, session windows)
- ‚ö†Ô∏è Event time processing with watermarks is needed
- ‚ö†Ô∏è Stateful aggregations across time windows
- ‚ö†Ô∏è Complex event processing (CEP) patterns

### Business Logic Placement

**‚ùå WRONG:**
```python
# DON'T put business logic in stream processors
class MyStreamProcessor:
    def process(self, event):
        if event['score'] > 0.7:  # ‚ùå Business logic
            event['severity'] = 'high'
```

**‚úÖ CORRECT:**
```python
# services/plagiarism_service/app/engine.py
async def run_plagiarism_pipeline(sub: Submission) -> PlagiarismResult:
    internal = check_internal_plagiarism()
    
    # ‚úÖ Business logic in service
    if internal >= 0.7:
        severity = "high"
    elif internal >= 0.4:
        severity = "medium"
    else:
        severity = "low"
    
    return PlagiarismResult(..., severity=severity)
```

### Stream Processor Template

**For simple consume/produce (use aiokafka):**
```python
# services/my_service/app/stream_processor.py
from aiokafka import AIOKafkaConsumer, AIOKafkaProducer

class MyStreamProcessor:
    async def process_messages(self):
        async for msg in self.consumer:
            event = json.loads(msg.value)
            # ‚úÖ Minimal enrichment only
            event['processed_at'] = datetime.utcnow().isoformat()
            await self.producer.send('output_topic', json.dumps(event).encode())
```

**For time-based windowing (use Flink):**
```python
# services/analytics_service/app/stream_processor.py
from pyflink.datastream.window import TumblingProcessingTimeWindows
from pyflink.common import Time

# ‚úÖ Flink for windowing
stream.window(TumblingProcessingTimeWindows.of(Time.minutes(5))) \
      .aggregate(MyAggregator())
```

## Technology Stack

### Core
- **FastAPI** - Web framework for microservices
- **Pydantic** - Data validation and event schemas
- **aiokafka** - Async Kafka client for stream processing
- **Apache Flink** - Time-based windowing (analytics only)
- **Uvicorn** - ASGI server

### Infrastructure
- **Apache Kafka** - Event streaming platform
- **PostgreSQL** - Relational database (users, submissions)
- **MongoDB** - Document database (plagiarism results)
- **Redis** - In-memory cache (analytics, notifications)

### Development
- **Pytest** - Testing framework
- **Jinja2** - Template engine (gateway)
- **Docker** - Containerization
- **Kubernetes** - Orchestration (k8s manifests included)

## Docker Deployment

The project includes a complete Docker Compose setup:

```bash
# Build and start all services
docker-compose up -d

# View logs
docker-compose logs -f [service-name]

# Stop all services
docker-compose down
```

**Infrastructure services included:**
- PostgreSQL (Port 5432)
- MongoDB (Port 27017)
- Redis (Port 6379)
- Zookeeper (Port 2181)
- Kafka (Port 9092)

**Application services:**
- All 6 microservices with embedded stream processors
- Stream processors start automatically with their parent services

### Monitoring

Check service health:
```bash
# Check all services
docker-compose ps

# View specific service logs
docker-compose logs -f analytics_service
```

## Service Ports

| Service | Port | Description |
|---------|------|-------------|
| Gateway | 8000 | Web UI and API gateway |
| User Service | 8001 | User management |
| Submission Service | 8002 | Paper submissions + stream processor |
| Plagiarism Service | 8003 | Plagiarism detection + stream processor |
| Analytics Service | 8004 | Analytics + Flink windowing |
| Notification Service | 8005 | Notifications |
| PostgreSQL | 5432 | Relational database |
| MongoDB | 27017 | Document database |
| Redis | 6379 | Cache |
| Zookeeper | 2181 | Kafka coordination |
| Kafka | 9092 | Event streaming |

## Future Enhancements

- [ ] Authentication/authorization (JWT-based)
- [ ] API versioning
- [ ] Health check endpoints for all services
- [ ] Distributed tracing (OpenTelemetry)
- [ ] Monitoring and alerting (Prometheus + Grafana)
- [ ] CI/CD pipeline
- [ ] Advanced Flink features (complex event processing patterns, session windows)
- [ ] ML model integration for plagiarism detection
- [ ] Real-time plagiarism score updates via WebSocket

---

**Complete architecture diagram available above** ‚¨ÜÔ∏è

#### Footer
The README references the assignment question file [251_CC_Assignment.txt](251_CC_Assignment.txt) to check for completion status.